{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from densenet import densenet121_attn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.misc import imresize\n",
    "import glob\n",
    "from visualize import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_img(img):\n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_mask(img, m1, m2, m3, title=None, filename=None):\n",
    "    \"\"\"\n",
    "    Plots the given torch tensor as grayscale mask\n",
    "    \"\"\"\n",
    "    _ = plt.figure(figsize = (10,10))\n",
    "    _ = plt.subplot(2,2,1)\n",
    "    _ = plt.axis('off')\n",
    "    if title is not None:\n",
    "        _ = plt.suptitle(title, verticalalignment='bottom', y=0.9)\n",
    "    _ = plt.imshow(img)\n",
    "    _ = plt.subplot(2,2,2)\n",
    "    _ = plt.axis('off')\n",
    "    _ = plt.imshow(img)\n",
    "    m = imresize(m1.data.numpy()[0], (299,299))\n",
    "    _ = plt.imshow(m, 'jet', interpolation='none', alpha=0.5)\n",
    "    _ = plt.subplot(2,2,3)\n",
    "    _ = plt.axis('off')\n",
    "    _ = plt.imshow(img)\n",
    "    m = imresize(m2.data.numpy()[0], (299,299))\n",
    "    _ = plt.imshow(m, 'jet', interpolation='none', alpha=0.5)\n",
    "    _ = plt.subplot(2,2,4)\n",
    "    _ = plt.axis('off')\n",
    "    _ = plt.imshow(img)\n",
    "    m = imresize(m3.data.numpy()[0], (299,299))\n",
    "    _ = plt.imshow(m, 'jet', interpolation='none', alpha=0.5)\n",
    "    _ = plt.show()\n",
    "    if filename is not None:\n",
    "        _ = plt.savefig(filename+'_mask.png')\n",
    "    \n",
    "def get_img_array(path, target_dim=(299,299)):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return img_to_array(load_img(path, target_size=target_dim))/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = '../saved/den_121_attn_cub_crop_13_0.80375.pth'\n",
    "d = densenet121_attn(weights=weights, mask_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict mask for random validation images\n",
    "# and visualize it\n",
    "data_dir = '/home/birdsnap/CUB_200_2011/cropped_test/validation'\n",
    "base_save_path = '../vis/1_simpleFCmaskPred/'\n",
    "folders = [file for file in glob.glob(data_dir+'*/*')]\n",
    "for folder in folders[:10]:\n",
    "    files = [file for file in glob.glob(folder + '/*')]\n",
    "    file_sample = np.random.choice(files, 3, replace=False)\n",
    "    sample = [get_img_array(i) for i in file_sample]\n",
    "    img_v = [torch.autograd.Variable(torch.Tensor(i)).view(1,299, 299, 3) for i in sample]\n",
    "    img_v = torch.cat(img_v, 0)\n",
    "    img_v = img_v.permute(0, 3, 2, 1)\n",
    "    m1, m2, m3 = d(img_v)\n",
    "    for i in range(3):\n",
    "        filename = folder.split('/')[-1] + '_' + file_sample[i].split('/')[-1]\n",
    "        full_path = base_save_path + filename\n",
    "        plot_mask(sample[i], m1[i], m2[i], m3[i], title=filename, filename=full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot sum of all masks\n",
    "m4 = m1 + m2 + m3\n",
    "i = 0\n",
    "plot_mask(sample[i], m1[i], m2[i], m4[i], title=\"Sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize computation graph\n",
    "make_dot(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsampler(torch.nn.Module):\n",
    "    def __init__(self, target_dim=(299,299), mode='bilinear'):\n",
    "\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.h = target_dim[0]\n",
    "        self.w = target_dim[1]\n",
    "\n",
    "        self.upsampler = torch.nn.Upsample(size=target_dim, mode=mode)\n",
    "        \n",
    "    def img_crop(self, x, tl_x, tl_y, br_x, br_y, target_size=(299,299)):\n",
    "        \"\"\"\n",
    "        Takes tensor of dimension x: (3, 299, 299) and\n",
    "        f: (s, 4) containing tl_x, tl_y, br_x, br_y in that\n",
    "        order. Returns upsampled crops\n",
    "        \"\"\"\n",
    "        # note that the following step is not \n",
    "        # a part of the network, taking values\n",
    "        # out of the tensor here\n",
    "        tl_x, tl_y, br_x, br_y = int(tl_x.data[0]), int(tl_y.data[0]), int(br_x.data[0]), int(br_y.data[0])\n",
    "        #cropped = img_set_zero(x, tl_x, tl_y, br_x, br_y)\n",
    "        #cropped = cropped[:,tl_x:br_x,tl_y:br_y].contiguous()\n",
    "        cropped = x[:,tl_x:br_x,tl_y:br_y].contiguous()\n",
    "        cropped = cropped.view(1, 3, cropped.size(1), cropped.size(2))\n",
    "        bi = torch.nn.Upsample(size=(299,299), mode='bilinear')\n",
    "        upped = bi(cropped).view(3, 299, 299)\n",
    "        return upped\n",
    "\n",
    "    def img_crops(self, x, f):\n",
    "        \"\"\"\n",
    "        x: (3, 299, 299)\n",
    "        f: (g, 4) tl_x, tl_y, br_x, br_y\n",
    "        returns cropped and upsampled same as x.size\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for f_i in torch.unbind(f):\n",
    "            out.append(self.img_crop(x, f_i[0], f_i[1], f_i[2], f_i[3]))\n",
    "        out = torch.stack(out, 0)\n",
    "        return out\n",
    "\n",
    "    def imgs_crops(self, x, f):\n",
    "        \"\"\"\n",
    "        x: (s, 3, 299, 299)\n",
    "        f: (s, g, 4) tl_x, tl_y, br_x, br_y\n",
    "        returns cropped and upsampled same as x.size\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for i,x_i in enumerate(torch.unbind(x)):\n",
    "            out.append(self.img_crops(x_i, f[i]))\n",
    "        out = torch.stack(out, 0)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, f):\n",
    "        return self.imgs_crops(img_v, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load two images into pyt vars\n",
    "img_path = '/home/birdsnap/CUB_200_2011/cropped_test/validation/200.Common_Yellowthroat/Common_Yellowthroat_0092_190573.jpg'\n",
    "img = get_img_array(img_path)\n",
    "print img.shape\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "img_v1 = torch.autograd.Variable(torch.Tensor(img).view(1,299,299,3)).permute(0, 3, 1, 2)\n",
    "\n",
    "img_path = '/home/birdsnap/CUB_200_2011/cropped_test/validation/200.Common_Yellowthroat/Common_Yellowthroat_0075_190900.jpg'\n",
    "img = get_img_array(img_path)\n",
    "plot_data_img(img)\n",
    "img_v2 = torch.autograd.Variable(torch.Tensor(img).view(1,299,299,3)).permute(0, 3, 1, 2)\n",
    "img_v = torch.cat([img_v1, img_v2], 0)\n",
    "print img_v.size()\n",
    "\n",
    "\n",
    "# 2 masks per image, 2 images: (s=2, g=2, 4)\n",
    "# 4 points in order: tl_x, tl_y, br_x, br_y\n",
    "\n",
    "f = torch.autograd.Variable(torch.Tensor([[[0, 20, 150, 150], [200, 245, 298, 298]], \n",
    "                                          [[50, 35, 275, 200], [200, 150, 280, 280]]]))\n",
    "print f.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = Upsampler()\n",
    "cropped = up(img_v, f)\n",
    "print cropped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.imshow(cropped[i][j].permute(1, 2, 0).data.numpy())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(1)\n",
    "j = torch.autograd.Variable(torch.randn(3, 2, 4)*10)\n",
    "print j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = j.clone()\n",
    "i[:, :, 2] = i[:, :, 2]*1000\n",
    "\n",
    "print i\n",
    "i[:, :, 2] = torch.clamp(i[:, :, 2], max=5)\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = i.long()\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = time.time() -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def save_plot(imgs, name):\n",
    "    \"\"\" \n",
    "    Takes a (299, 299, 3)\n",
    "    Glimpses: (3, 3, 299, 299)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    n = len(imgs)\n",
    "    x = math.ceil(n/2)\n",
    "    y = 2\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        plt.subplot(x,y,i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "    plt.savefig(name)\n",
    "\n",
    "def save_glimpses(x, glimpses, epoch, path, exp_name):\n",
    "    \"\"\" \n",
    "    x: (s, 3, 299, 299)\n",
    "    glimpses: (s, g, 3, 299, 299)\n",
    "    \"\"\"\n",
    "    glimpses = glimpses.permute(0, 1, 3, 4, 2)\n",
    "    glimpses = glimpses.data.numpy()\n",
    "    x = x.permute(0, 2, 3, 1).data.numpy()\n",
    "    for s in range(x.shape[0]):\n",
    "        name = \"{}/{}_{}.png\".format(path, exp_name, epoch)\n",
    "        img = x[s]\n",
    "        save_plot([img, glimpses[s][0], glimpses[s][1], glimpses[s][2]], name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-49480c662be2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msave_glimpses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-c5aa5fed900f>\u001b[0m in \u001b[0;36msave_glimpses\u001b[0;34m(x, glimpses, epoch, path, exp_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}/{}_{}.png\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msave_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglimpses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglimpses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglimpses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "path = 'visual_attn/vis/2_RACNN'\n",
    "x = torch.autograd.Variable(torch.randn(2,3,299,299))\n",
    "i = torch.autograd.Variable(torch.randn(2,2,3,299,299))\n",
    "save_glimpses(x, i, 3, path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
